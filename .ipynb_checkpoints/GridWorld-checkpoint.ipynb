{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, rows, cols, start, goal):\n",
    "        \n",
    "        self.env = np.zeros((rows,cols))\n",
    "        \n",
    "        self.rows = rows\n",
    "        \n",
    "        self.cols = cols\n",
    "        \n",
    "        self.start = start \n",
    "        \n",
    "        self.goal = goal\n",
    "        \n",
    "        self.env[goal] = 100\n",
    "        \n",
    "        self.weights = np.random.uniform(1,100,2)\n",
    "        \n",
    "        # return a 2d array for transpose\n",
    "        self.feature = lambda s,a: np.array([self.nextState(s,a)])\n",
    "        \n",
    "        self.qvalue = lambda s,a: ((self.feature(s,a))).dot(self.weights.T)\n",
    "        \n",
    "        self.beta = np.random.uniform(0,1)\n",
    "        \n",
    "    def reward(self, state, action):\n",
    "        \n",
    "        if(action == 0):\n",
    "            \n",
    "            if(state[0] == 0):\n",
    "                return self.env[state[0], state[1]] - 1\n",
    "            \n",
    "            return self.env[state[0] - 1, state[1]]\n",
    "        \n",
    "        elif(action == 1):\n",
    "            \n",
    "            if(state[0] == self.cols - 1):\n",
    "                return self.env[state[0],state[1]] - 1 \n",
    "            \n",
    "            return self.env[state[0] + 1, state[1]]\n",
    "        \n",
    "        elif(action == 2):\n",
    "            \n",
    "            if(state[1] == 0):\n",
    "                return self.env[state[0],state[1]] - 1\n",
    "            \n",
    "            return self.env[state[0], state[1] - 1]\n",
    "        \n",
    "        elif(action == 3):\n",
    "        \n",
    "            if(state[1] == self.rows - 1):\n",
    "                return self.env[state[0], state[1]] - 1\n",
    "        \n",
    "            return self.env[state[0], state[1] + 1]\n",
    "        \n",
    "        elif(action == 4):\n",
    "            return self.env[state[0], state[1]]\n",
    "        \n",
    "    def nextState(self, state, action):\n",
    "        \n",
    "        if(action == 0):\n",
    "            \n",
    "            if(state[0] == 0):\n",
    "                return [state[0], state[1]]\n",
    "            \n",
    "            return [state[0] - 1, state[1]]\n",
    "        \n",
    "        elif(action == 1):\n",
    "            \n",
    "            if(state[0] == self.cols - 1):\n",
    "                return [state[0],state[1]]\n",
    "            \n",
    "            return [state[0] + 1, state[1]]\n",
    "        \n",
    "        elif(action == 2):\n",
    "            \n",
    "            if(state[1] == 0):\n",
    "                return [state[0],state[1]]\n",
    "            \n",
    "            return [state[0], state[1] - 1]\n",
    "        \n",
    "        elif(action == 3):\n",
    "        \n",
    "            if(state[1] == self.rows - 1):\n",
    "                return [state[0], state[1]]\n",
    "        \n",
    "            return [state[0], state[1] + 1]\n",
    "        \n",
    "        elif(action == 4):\n",
    "            return [state[0], state[1]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, rows, cols, start):\n",
    "        \n",
    "        self.parameter = np.random.uniform(1,100,2)\n",
    "        \n",
    "        self.rows = rows\n",
    "        \n",
    "        self.cols = cols\n",
    "        \n",
    "        self.actions = ['up','down','left','right','stay']\n",
    "        \n",
    "        self.feature = lambda s,a: np.array([self.nextState(s,a)])    \n",
    "               \n",
    "        self.sample_action = lambda s: np.random.choice(self.actions,p=self.policy_model(s))\n",
    "       \n",
    "        self.gamma = np.random.uniform(0,1)\n",
    "        \n",
    "        self.alpha = np.random.uniform(0,1)\n",
    "        \n",
    "        self.start = start\n",
    "        \n",
    "        \n",
    "    def policy_model(self, s):\n",
    "        \n",
    "        probs = np.array([])\n",
    "        \n",
    "        for i in range(len(self.actions)):\n",
    "            \n",
    "            probs = np.hstack((probs, np.exp(((self.feature(s,i))).dot(self.parameter.T))[0]))\n",
    "            \n",
    "            \n",
    "        probs = probs/np.sum(probs, axis=0)\n",
    "        \n",
    "        \n",
    "        return probs\n",
    "            \n",
    "        \n",
    "    def score_fn(self, state, action):\n",
    "            \n",
    "        avg = np.zeros((1,2))    \n",
    "        \n",
    "        probs = self.policy_model(state)\n",
    "        \n",
    "        for i in range(len(probs)):\n",
    "                \n",
    "                avg += self.feature(state,i) * probs[i]\n",
    "        \n",
    "        return self.feature(state,action) - avg\n",
    "    \n",
    "    \n",
    "    def nextState(self, state, action):\n",
    "        \n",
    "        if(action == 0):\n",
    "            \n",
    "            if(state[0] == 0):\n",
    "                return [state[0], state[1]]\n",
    "            \n",
    "            return [state[0] - 1, state[1]]\n",
    "        \n",
    "        elif(action == 1):\n",
    "            \n",
    "            if(state[0] == self.cols - 1):\n",
    "                return [state[0],state[1]]\n",
    "            \n",
    "            return [state[0] + 1, state[1]]\n",
    "        \n",
    "        elif(action == 2):\n",
    "            \n",
    "            if(state[1] == 0):\n",
    "                return [state[0],state[1]]\n",
    "            \n",
    "            return [state[0], state[1] - 1]\n",
    "        \n",
    "        elif(action == 3):\n",
    "        \n",
    "            if(state[1] == self.rows - 1):\n",
    "                return [state[0], state[1]]\n",
    "        \n",
    "            return [state[0], state[1] + 1]\n",
    "        \n",
    "        elif(action == 4):\n",
    "            return [state[0], state[1]]\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = GridWorld(5,5,(0,0),(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(5,5,(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAC():\n",
    "    \n",
    "    s = environment.start\n",
    "    \n",
    "    a = agent.sample_action(s)\n",
    "    \n",
    "    print(a)\n",
    "    \n",
    "    a = agent.actions.index(a)\n",
    "    \n",
    "    print(a)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "#     for i in range(1000):\n",
    "        \n",
    "#         count = 0\n",
    "#         s = environment.start\n",
    "    \n",
    "    while s[0] != 4 or s[1] != 4:\n",
    "\n",
    "#         if(count == 100):\n",
    "#             break\n",
    "\n",
    "        r = environment.reward(s,a)\n",
    "\n",
    "#         print(r)\n",
    "\n",
    "        s1 = environment.nextState(s, a)\n",
    "\n",
    "        print(s1)\n",
    "\n",
    "        a1 = agent.sample_action(s1)\n",
    "\n",
    "        a1 = agent.actions.index(a1)\n",
    "\n",
    "#             print(a1)\n",
    "\n",
    "        td_error = r + (agent.gamma * (environment.qvalue(s1,a1))) - environment.qvalue(s,a)\n",
    "\n",
    "\n",
    "#         print(td_error)\n",
    "\n",
    "        agent.parameter += agent.alpha * (agent.score_fn(s,a)[0]) * (environment.qvalue(s,a)[0])\n",
    "\n",
    "#         print(agent.parameter)\n",
    "\n",
    "        environment.weights += environment.beta * (td_error) * (environment.feature(s,a)[0])\n",
    "\n",
    "#         print(environment.weights)\n",
    "\n",
    "        a = a1\n",
    "\n",
    "        s = s1\n",
    "\n",
    "#         count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n",
      "1\n",
      "[1, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[4, 1]\n",
      "[4, 2]\n",
      "[4, 3]\n",
      "[4, 4]\n"
     ]
    }
   ],
   "source": [
    "QAC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "final_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_policy():\n",
    "    \n",
    "    out = np.zeros((environment.rows, environment.cols))\n",
    "\n",
    "    for i in range(environment.rows):\n",
    "        \n",
    "        for j in range(environment.cols):\n",
    "\n",
    "#             print(agent.policy_model([i,j]))\n",
    "                \n",
    "            out[i][j] = np.argmax(agent.policy_model([i,j]))\n",
    "    \n",
    "    print(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.52800876e+11 2.49290774e+15 2.49290774e+15 4.38479211e+17\n",
      " 2.49290774e+15] 3\n"
     ]
    }
   ],
   "source": [
    "probs = np.array([])\n",
    "\n",
    "for i in range(len(agent.actions)):\n",
    "    \n",
    "    probs = np.hstack((probs, np.exp(((agent.feature([4,0],i))).dot(agent.parameter.T))[0]))\n",
    "\n",
    "print(probs,np.argmax(probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "ai-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
